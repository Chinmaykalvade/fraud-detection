{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CCFD neural code.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfDbN5puywEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Import and store dataset\n",
        "credit_card_data = pd.read_csv('creditcard.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjlIvcxvzC9g",
        "colab_type": "text"
      },
      "source": [
        "  Data@https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
        "\tSplitting data into 4 sets\n",
        "1. Shuffle/randomize data\n",
        "2. One-hot encoding\n",
        "3. Normalize\n",
        "4. Splitting up X/y values\n",
        "5. Convert data_frames to numpy arrays (float32)\n",
        "6. Splitting the final data into X/y train/test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASCsDms4zaqo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle and randomize data\n",
        "shuffled_data = credit_card_data.sample(frac=1)\n",
        "#\tChange Class column into Class_0 ([1 0] for legit data) and Class_1 ([0 1] for fraudulent data)\n",
        "one_hot_data = pd.get_dummies(shuffled_data, columns=['Class'])\n",
        "#\tChange all values into numbers between 0 and 1\n",
        "normalized_data = (one_hot_data - one_hot_data.min()) / (one_hot_data.max() - one_hot_data.min())\n",
        "#\tStore just columns V1 through V28 in df_X and columns Class_0 and Class_1 in df_y\n",
        "df_X = normalized_data.drop(['Class_0', 'Class_1'], axis=1)\n",
        "df_y = normalized_data[['Class_0', 'Class_1']]\n",
        "#\tConvert both data_frames into np arrays of float32\n",
        "ar_X, ar_y = np.asarray(df_X.values, dtype='float32'), np.asarray(df_y.values, dtype='float32')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEcFwThT0Dly",
        "colab_type": "code",
        "outputId": "e1fe3bbb-612d-46bd-eaa6-755451293582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#\tAllocate first 80% of data into training data and remaining 20% into testing data\n",
        "train_size = int(0.8 * len(ar_X))\n",
        "(raw_X_train, raw_y_train) = (ar_X[:train_size], ar_y[:train_size])\n",
        "(raw_X_test, raw_y_test) = (ar_X[train_size:],ar_y[train_size:])\n",
        "#\tGets a percent of fraud vs legit transactions (0.0017% of transactions are fraudulent)\n",
        "count_legit, count_fraud = np.unique(credit_card_data['Class'], return_counts=True)[1]\n",
        "fraud_ratio = float(count_fraud / (count_legit + count_fraud))\n",
        "print('Percent of fraudulent transactions: ', fraud_ratio)\n",
        "#\tApplies a logic weighting of 578 (1/0.0017) to fraudulent transactions to cause model to pay more attention to them\n",
        "weighting = 1 / fraud_ratio\n",
        "raw_y_train[:, 1] = raw_y_train[:, 1] * weighting"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percent of fraudulent transactions:  0.001727485630620034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR_l-MvO0Xh7",
        "colab_type": "text"
      },
      "source": [
        "Contructing the neural network using tensorflow...\n",
        "here, we require:\n",
        "\n",
        "100 cells for the 1st layer num_layer_1_cells = 100\n",
        "\n",
        "150 cells for the second layer num_layer_2_cells = 150\n",
        "\n",
        "We will use these as inputs to the model when it comes time to train it (assign values at run time)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDmIN9tm0sZw",
        "colab_type": "code",
        "outputId": "b2cd6c63-b350-45c8-8f85-e6498173b373",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "# 30 cells for the input\n",
        "input_dimensions = ar_X.shape[1]\n",
        "# 2 cells for the output\n",
        "output_dimensions = ar_y.shape[1]\n",
        "X_train_node = tf.placeholder(tf.float32, [None, input_dimensions], name='X_train')\n",
        "y_train_node = tf.placeholder(tf.float32, [None, output_dimensions], name='y_train')\n",
        "#\tWe will use these as inputs to the model once it comes time to test it\n",
        "X_test_node = tf.constant(raw_X_test, name='X_test')\n",
        "y_test_node = tf.constant(raw_y_test, name='y_test')\n",
        "#\tFirst layer takes in input and passes output to 2nd layer\n",
        "weight_1_node = tf.Variable(tf.zeros([input_dimensions, num_layer_1_cells]), name='weight_1')\n",
        "biases_1_node = tf.Variable(tf.zeros([num_layer_1_cells]), name='biases_1')\n",
        "#\tSecond layer takes in input from 1st layer and passes output to 3rd layer\n",
        "weight_2_node = tf.Variable(tf.zeros([num_layer_1_cells, num_layer_2_cells]), name='weight_2')\n",
        "biases_2_node = tf.Variable(tf.zeros([num_layer_2_cells]), name='biases_2')\n",
        "#\tThird layer takes in input from 2nd layer and outputs [1 0] or [0 1] depending on fraud vs legit\n",
        "weight_3_node = tf.Variable(tf.zeros([num_layer_2_cells, output_dimensions]), name='weight_3')\n",
        "biases_3_node = tf.Variable(tf.zeros([output_dimensions]), name='biases_3')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6e7755e65651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my_test_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#       First layer takes in input and passes output to 2nd layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mweight_1_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layer_1_cells\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weight_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mbiases_1_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_layer_1_cells\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'biases_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#       Second layer takes in input from 1st layer and passes output to 3rd layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'num_layer_1_cells' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTQptffk1jMg",
        "colab_type": "text"
      },
      "source": [
        "Function to run an input tensor through the 3 layers and output a tensor that will give us a fraud/legit result\n",
        "\n",
        "Each layer uses a different function to fit lines through the data and predict whether a given input tensor will result in a fraudulent or legitimate transaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iDgP0An1qg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def network(input_tensor):\n",
        "# Sigmoid fits modified data well\n",
        " layer1= tf.nn.sigmoid(tf.matmul(input_tensor, weight_1_node) + biases_1_node)\n",
        "#\tDropout prevents model from becoming lazy and over confident\n",
        " layer2 = tf.nn.dropout(tf.nn.sigmoid(tf.matmul(layer1, weight_2_node) + biases_2_node), 0.85)\n",
        "#\tSoftmax works very well with one hot encoding which is how results are outputted\n",
        " layer3 = tf.nn.softmax(tf.matmul(layer2, weight_3_node) + biases_3_node)\n",
        "  return layer3\n",
        "#\tUsed to predict what results will be given training or testing input data\n",
        "#\tRemember, X_train_node is just a placeholder for now. We will enter values at run time\n",
        "y_train_prediction = network(X_train_node)\n",
        "y_test_prediction = network(X_test_node)\n",
        "#\tCross entropy loss function measures differences between actual output and predicted output\n",
        "cross_entropy = tf.losses.softmax_cross_entropy(y_train_node, y_train_prediction)\n",
        "#\tAdam optimizer function will try to minimize loss (cross_entropy) but changing the 3 layers' variable values at alearning rate of 0.005\n",
        "optimizer = tf.train.AdamOptimizer(0.005).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb1sKBgO15S8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\tFunction to calculate the accuracy of the actual result vs the predicted result\n",
        "def calculate_accuracy(actual, predicted):\n",
        "  actual = np.argmax(actual, 1)\n",
        "  predicted = np.argmax(predicted, 1)\n",
        "  return (100 * np.sum(np.equal(predicted, actual)) / predicted.shape[0])\n",
        "num_epochs = 100\n",
        "import time\n",
        "with tf.Session() as session:\n",
        "  tf.global_variables_initializer().run()\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  cross_entropy_score = session.run([optimizer, cross_entropy],\n",
        "  feed_dict={X_train_node:raw_X_train, y_train_node: raw_y_train})\n",
        "if (epoch % 10 == 0):\n",
        "  timer = time.time() - start_time\n",
        "  print('Epoch: {}'.format(epoch), 'Current loss:{0:.4f}'.format(cross_entropy_score),'Elapsed time: {0:.2f} seconds'.format(timer))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PepUciw62Qqn",
        "colab_type": "text"
      },
      "source": [
        "#Final evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rplmwZAw113w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_y_test = y_test_node.eval()\n",
        "final_y_test_prediction =y_test_prediction.eval()\n",
        "final_accuracy = calculate_accuracy(final_y_test, final_y_test_prediction)\n",
        "print(\"Current accuracy:{0:.2f}%\".format(final_accuracy))\n",
        "final_y_test = y_test_node.eval()\n",
        "final_y_test_prediction = y_test_prediction.eval()\n",
        "final_accuracy = calculate_accuracy(final_y_test,final_y_test_prediction)\n",
        "print(\"Final accuracy:{0:.2f}%\".format(final_accuracy))\n",
        "final_fraud_y_test = final_y_test[final_y_test[:, 1] == 1]\n",
        "final_fraud_y_test_prediction =final_y_test_prediction[final_y_test[:, 1] == 1]\n",
        "final_fraud_accuracy =calculate_accuracy(final_fraud_y_test,final_fraud_y_test_prediction)\n",
        "print('Final fraud specific accuracy:{0:.2f}%'.format(final_fraud_accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}